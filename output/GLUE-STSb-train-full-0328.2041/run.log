/home/chris/miniconda3/envs/DeepKorean/bin/python /dat/proj/DeepKorean/src/train.py --config conf/GLUE-STSb-train-full.json
Global seed set to 1
================================================================================================================================================================================================================================================================
[2022/03/28 20:41:45] [INIT] train (GLUE-STSb-train-full-0328.2041)
================================================================================================================================================================================================================================================================
- state.name                = (str) GLUE-STSb-train-full-0328.2041
- state.seed                = (int) 1
- state.gpus                = (list) [1]
- state.strategy            = (NoneType) None
- state.precision           = (int) 32
- state.test_batch_size     = (int) -1
- state.train_batch_size    = (int) -1
- state.max_sequence_length = (int) 128
- state.pretrained          = (str) pretrained/BERT-Base-mul-unc
- state.data_files          = (dict) {'train': 'data/glue-stsb/train.json', 'valid': 'data/glue-stsb/validation.json', 'test': None}
- state.input_text1         = (str) sentence1
- state.input_text2         = (str) sentence2
- state.loss_metric         = (str) MSELoss
- state.score_metric        = (dict) {'major': 'glue', 'minor': 'stsb'}
- state.num_train_epochs    = (int) 5
- state.scheduling_epochs   = (int) 1
- state.scheduling_gamma    = (float) 1.0
- state.learning_rate       = (float) 2e-05
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- pretrained_tokenizer      = BertTokenizerFast / is_fast=True / vocab_size=105879 / model_input_names=input_ids, token_type_ids, attention_mask / model_max_length=128 / padding_side=right / special_tokens=[UNK], [SEP], [PAD], [CLS], [MASK]
- tokenized.sample.plain    = [CLS] 한국어 사전학습 모델을 공유합니다. [SEP]
- tokenized.sample.morps    = [CLS] 한국어/NNP 사전/NNG 학습/NNG 모델/NNG 을/JKO 공유/NNG 하/XSV ㅂ니다/EF ./SF [SEP]
- tokenized.sample.tks      = [CLS] 한국 ##어 ᄉ ##ᅡ ##전 ##학 ##스 ##ᆸ ᄆ ##ᅩ ##데 ##ᆯ을 고 ##ᆼ ##유 ##합 ##니다 . [SEP]
- tokenized.sample.ids      = 101 47529 13413 1172 25539 17101 21596 13212 17360 1169 29347 26872 22058 47468 13045 42159 44859 82566 119 102
- tokenized.examples[0].tks = [CLS] a plane is taking off . [SEP] an air plane is taking off . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
- tokenized.examples[0].ids = 101 143 24488 10127 17301 11856 119 102 10144 11140 24488 10127 17301 11856 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0
- tokenized.examples[1].tks = [CLS] a man is playing a large flute . [SEP] a man is playing a flute . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
- tokenized.examples[1].ids = 101 143 10564 10127 14734 143 12166 43460 119 102 143 10564 10127 14734 143 43460 119 102 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0
- tokenized.examples[2].tks = [CLS] a man is spread ##ing sh ##rede ##d cheese on a pizza . [SEP] a man is spread ##ing sh ##red ##ded cheese on an un ##co ##oke ##d ... [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
- tokenized.examples[2].ids = 101 143 10564 10127 24558 10285 23015 20880 10163 66670 10125 143 59371 119 102 143 10564 10127 24558 10285 23015 15713 20298 66670 10125 10144 10119 10805 30037 10163 ... 0 0 0 0 0 0 0 0 0 0
- raw_datasets[train]       =   5,749 samples / label[float64], input_ids[list], token_type_ids[list], attention_mask[list]
- raw_datasets[valid]       =   1,500 samples / label[float64], input_ids[list], token_type_ids[list], attention_mask[list]
- input_text_columns        = sentence1, sentence2
- label_column              = label
- state.train_batch_size    = (int) 96
- state.test_batch_size     = (int) 96
Some weights of the model checkpoint at pretrained/BERT-Base-mul-unc were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- pretrained_model          = BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(105879, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  ...
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)
- encoded.input_ids         = (2x128) / 101 47529 13413 1172 25539 17101 21596 13212 17360 1169 29347 26872 22058 47468 13045 42159 44859 82566 119 102 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
- encoded.attention_mask    = (2x128) / 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
- encoded.token_type_ids    = (2x128) / 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
- forwarded.hidden_output   = (2x128x768) / tensor([[-0.0510, -0.1183, -0.0165,  ...,  0.0060, -0.0585, -0.1084],
        [ 0.3172,  0.0977, -0.1539,  ...,  0.0561,  0.4085, -0.9275],
        [ 0.2552, -0.2561, -0.2972,  ..., -0.2263,  0.8786, -0.2729],
        ...,
        [ 0.0565, -0.0844, -0.0249,  ..., -0.2546,  0.3479, -0.2610],
        [ 0.2467, -0.2577,  0.3330,  ...,  0.0905,  0.2164,  0.1767],
        [ 0.1810, -0.0716, -0.0309,  ...,  0.2375, -0.2798, -0.0863]],
       grad_fn=<SelectBackward0>)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
- model                     = MyModel(pretrained=BERT-Base-mul-unc)
- optimizer                 = Adam(lr=2e-05)
- scheduler                 = StepLR(step_size=1, gamma=1.0)
- loss_metric               = MSELoss()
- score_metric              = load_metric(glue, stsb)
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
================================================================================================================================================================================================================================================================
[2022/03/28 20:42:00] (Epoch 01) composed #1: learning_rate=0.0000200000
[2022/03/28 20:42:04] (Epoch 01) training #1: 100%|██████████████████████████████| 60/60 [00:36<00:00,  1.65it/s]
[2022/03/28 20:42:45] (Epoch 01) metering #1: 100%|██████████████████████████████| 60/60 [00:11<00:00,  5.20it/s]
[2022/03/28 20:42:57] (Epoch 01) metering #1: 100%|██████████████████████████████| 16/16 [00:02<00:00,  5.66it/s]
[2022/03/28 20:43:06] (Epoch 01) measured #1: step  | loss=2.0360, pearson=0.4273, spearmanr=0.4543, runtime=36.2876
[2022/03/28 20:43:06] (Epoch 01) measured #1: train | loss=0.8605, pearson=0.7931, spearmanr=0.7862, runtime=11.5453
[2022/03/28 20:43:06] (Epoch 01) measured #1: valid | loss=0.9776, pearson=0.7903, spearmanr=0.8160, runtime=2.8280
[2022/03/28 20:43:08] (Epoch 01) exported #1: model | output/GLUE-STSb-train-full-0328.2041/pytorch_model-01e
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[2022/03/28 20:43:11] (Epoch 02) composed #1: learning_rate=0.0000200000
[2022/03/28 20:43:15] (Epoch 02) training #1: 100%|██████████████████████████████| 60/60 [00:36<00:00,  1.64it/s]
[2022/03/28 20:43:55] (Epoch 02) metering #1: 100%|██████████████████████████████| 60/60 [00:11<00:00,  5.12it/s]
[2022/03/28 20:44:08] (Epoch 02) metering #1: 100%|██████████████████████████████| 16/16 [00:02<00:00,  5.58it/s]
[2022/03/28 20:44:16] (Epoch 02) measured #1: step  | loss=0.7142, pearson=0.8169, spearmanr=0.7850, runtime=36.5027
[2022/03/28 20:44:16] (Epoch 02) measured #1: train | loss=0.6102, pearson=0.8662, spearmanr=0.8555, runtime=11.7246
[2022/03/28 20:44:16] (Epoch 02) measured #1: valid | loss=0.7629, pearson=0.8359, spearmanr=0.8382, runtime=2.8684
[2022/03/28 20:44:18] (Epoch 02) exported #1: model | output/GLUE-STSb-train-full-0328.2041/pytorch_model-02e
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[2022/03/28 20:44:21] (Epoch 03) composed #1: learning_rate=0.0000200000
[2022/03/28 20:44:25] (Epoch 03) training #1: 100%|██████████████████████████████| 60/60 [00:36<00:00,  1.64it/s]
[2022/03/28 20:45:06] (Epoch 03) metering #1: 100%|██████████████████████████████| 60/60 [00:11<00:00,  5.09it/s]
[2022/03/28 20:45:19] (Epoch 03) metering #1: 100%|██████████████████████████████| 16/16 [00:02<00:00,  5.53it/s]
[2022/03/28 20:45:27] (Epoch 03) measured #1: step  | loss=0.5588, pearson=0.8601, spearmanr=0.8316, runtime=36.5593
[2022/03/28 20:45:27] (Epoch 03) measured #1: train | loss=0.5652, pearson=0.8957, spearmanr=0.8846, runtime=11.7969
[2022/03/28 20:45:27] (Epoch 03) measured #1: valid | loss=0.6897, pearson=0.8581, spearmanr=0.8589, runtime=2.8926
[2022/03/28 20:45:29] (Epoch 03) exported #1: model | output/GLUE-STSb-train-full-0328.2041/pytorch_model-03e
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[2022/03/28 20:45:32] (Epoch 04) composed #1: learning_rate=0.0000200000
[2022/03/28 20:45:36] (Epoch 04) training #1: 100%|██████████████████████████████| 60/60 [00:36<00:00,  1.63it/s]
[2022/03/28 20:46:17] (Epoch 04) metering #1: 100%|██████████████████████████████| 60/60 [00:11<00:00,  5.06it/s]
[2022/03/28 20:46:30] (Epoch 04) metering #1: 100%|██████████████████████████████| 16/16 [00:02<00:00,  5.50it/s]
[2022/03/28 20:46:39] (Epoch 04) measured #1: step  | loss=0.4831, pearson=0.8803, spearmanr=0.8539, runtime=36.7572
[2022/03/28 20:46:39] (Epoch 04) measured #1: train | loss=0.4058, pearson=0.9268, spearmanr=0.9167, runtime=11.8775
[2022/03/28 20:46:39] (Epoch 04) measured #1: valid | loss=0.6092, pearson=0.8746, spearmanr=0.8738, runtime=2.9103
[2022/03/28 20:46:41] (Epoch 04) exported #1: model | output/GLUE-STSb-train-full-0328.2041/pytorch_model-04e
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[2022/03/28 20:46:44] (Epoch 05) composed #1: learning_rate=0.0000200000
[2022/03/28 20:46:48] (Epoch 05) training #1: 100%|██████████████████████████████| 60/60 [00:36<00:00,  1.64it/s]
[2022/03/28 20:47:28] (Epoch 05) metering #1: 100%|██████████████████████████████| 60/60 [00:11<00:00,  5.04it/s]
[2022/03/28 20:47:41] (Epoch 05) metering #1: 100%|██████████████████████████████| 16/16 [00:02<00:00,  5.49it/s]
[2022/03/28 20:47:50] (Epoch 05) measured #1: step  | loss=0.3595, pearson=0.9123, spearmanr=0.8911, runtime=36.5930
[2022/03/28 20:47:50] (Epoch 05) measured #1: train | loss=0.2726, pearson=0.9425, spearmanr=0.9344, runtime=11.9136
[2022/03/28 20:47:50] (Epoch 05) measured #1: valid | loss=0.5849, pearson=0.8708, spearmanr=0.8694, runtime=2.9132
[2022/03/28 20:47:52] (Epoch 05) exported #1: model | output/GLUE-STSb-train-full-0328.2041/pytorch_model-05e
================================================================================================================================================================================================================================================================
[2022/03/28 20:47:55] [EXIT] train (GLUE-STSb-train-full-0328.2041) ($=00:06:09.742)
================================================================================================================================================================================================================================================================

Process finished with exit code 0
